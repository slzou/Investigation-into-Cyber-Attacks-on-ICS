{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn import tree\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/Pickles\"\n",
    "pickleFolder=os.getcwd()+path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataAtck=pd.read_pickle(pickleFolder+\"/BaseAttackDataframe.pkl\")\n",
    "dataNorm=pd.read_pickle(pickleFolder+\"/BaseNormalDataframe.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuator_list=[\"MV101\",\"P101\",\"P102\",\"MV201\",\"P201\",\"P202\",\"P203\",\"P204\",\"P205\",\"P206\",\"MV301\",\"MV302\",\"MV303\",\"MV304\",\"P301\",\"P302\",\"P401\",\"P402\",\"P403\",\"P404\",\"P501\",\"P502\",\"P601\",\"P602\",\"P603\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_actuator_list=[\"MV101\",\"P101\",\"MV201\",\"P203\",\"P205\",\"MV301\",\"MV302\",\"MV303\",\"MV304\",\"P301\",\"P302\",\"P402\",\"P501\",\"P602\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inactive_actuator_list=[\"P102\",\"P201\",\"P202\",\"P204\",\"P206\",\"P401\",\"P403\",\"P404\",\"P502\",\"P601\",\"P603\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General regression dataframe pickles\n",
    "\n",
    "dataAtck_reg_adpt=pd.read_pickle(pickleFolder+\"/GeneralRegressionDataframeAttack.pkl\")\n",
    "dataNorm_reg_adpt=pd.read_pickle(pickleFolder+\"/GeneralRegressionDataframeNormal.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression dataframe pickles\n",
    "\n",
    "AY_linreg=pd.read_pickle(pickleFolder+\"/LinearRegressionDataframeAttack.pkl\")\n",
    "Y_linreg=pd.read_pickle(pickleFolder+\"/LinearRegressionDataframeNormal.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_linreg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharding/miniconda3/lib/python3.7/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.20.2 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "with open(pickleFolder+'/new_d_log.pickle', 'rb') as handle:\n",
    "    d_log = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MV101\n",
      "P101\n",
      "MV201\n",
      "P203\n",
      "P205\n",
      "MV301\n",
      "MV302\n",
      "MV303\n",
      "MV304\n",
      "P301\n",
      "P302\n",
      "P402\n",
      "P501\n",
      "P602\n"
     ]
    }
   ],
   "source": [
    "p_value_log=pd.DataFrame([],columns=active_actuator_list)\n",
    "for p in active_actuator_list:\n",
    "    try:\n",
    "        kasia=np.multiply(np.array([np.array([0,1,2])==x for x in dataAtck_reg_adpt[p]]),d_log[\"classifier_{0}\".format(p)].predict_proba(dataAtck_reg_adpt.drop([p],axis=1)))\n",
    "        p_value_log[p]=sum(np.transpose(kasia))\n",
    "    except:\n",
    "        kasia=np.multiply(np.array([np.array([0,1])==x for x in dataAtck_reg_adpt[p]]),d_log[\"classifier_{0}\".format(p)].predict_proba(dataAtck_reg_adpt.drop([p],axis=1)))\n",
    "        p_value_log[p]=sum(np.transpose(kasia))\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalue_dataframe=pd.DataFrame([],columns=actuator_list)\n",
    "for x in active_actuator_list:\n",
    "    A=np.array(p_value_log[x])\n",
    "    B=np.multiply(d_log[\"classifier_{0}\".format(x)].predict_proba(dataAtck_reg_adpt.drop([x],axis=1)),d_log[\"classifier_{0}\".format(x)].predict_proba(dataAtck_reg_adpt.drop([x],axis=1))<A.reshape(len(A),1))\n",
    "    C=sum(np.transpose(B))\n",
    "    D=C+0.5*A\n",
    "    if not all(D>0):\n",
    "        D=D+0.000000001\n",
    "    pvalue_dataframe[x]=D\n",
    "Q=np.array([0.5,0.000000001])\n",
    "for x in inactive_actuator_list:\n",
    "    pvalue_dataframe[x]=Q[dataAtck_reg_adpt[x]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine P-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedValue=np.array([])\n",
    "for x in pvalue_dataframe.values:\n",
    "    combinedValue=np.hstack((combinedValue,sum(np.log(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalue_dataframe[\"Combined\"]=-2*combinedValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalue_dataframe_combineTime=pvalue_dataframe.copy()\n",
    "#for t in range(0,len(pvalue_dataframe[\"Combined\"])):\n",
    "#    pvalue_dataframe_combineTime[\"Combined\"][t]=sum(pvalue_dataframe[\"Combined\"][max(0,t-20):t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=open(\"/home/sharding/Documents/SummerProject/Data/AttackList_Indexed.csv\",\"r+\")\n",
    "file_atcklist=csv.reader(h)\n",
    "top=next(file_atcklist)\n",
    "attack_list=pd.DataFrame(file_atcklist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base=time.strptime('12/28/2015 10:00:00', '%m/%d/%Y %H:%M:%S')\n",
    "attack_start_times=[]\n",
    "for t in range(0,len(attack_list)):\n",
    "    attack_start_times.append(int(time.mktime(time.strptime(attack_list.iloc[t][1], '%m/%d/%Y %H:%M:%S'))-time.mktime(base)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=open(\"/home/sharding/Documents/SummerProject/Data/AttackList_Indexed.csv\",\"r+\")\n",
    "file_atcklist=csv.reader(h)\n",
    "top=next(file_atcklist)\n",
    "attack_list=pd.DataFrame(file_atcklist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail=120\n",
    "base=time.strptime('12/28/2015 10:00:00', '%m/%d/%Y %H:%M:%S')\n",
    "attack_start_times=[]\n",
    "for t in range(0,len(attack_list)):\n",
    "    attack_start_times.append(int(time.mktime(time.strptime(attack_list.iloc[t][1], '%m/%d/%Y %H:%M:%S'))-time.mktime(base)))\n",
    "attack_start_times=np.array(attack_start_times)\n",
    "attack_end_times=[]\n",
    "for t in range(0,len(attack_list)):\n",
    "    attack_end_times.append(int(time.mktime(time.strptime(attack_list.iloc[t][2], '%m/%d/%Y %H:%M:%S'))-time.mktime(base)+tail))\n",
    "attack_end_times=np.array(attack_end_times)\n",
    "attack_bins=np.transpose(np.vstack((attack_start_times,attack_end_times)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_pvalues=pvalue_dataframe_combineTime.copy()\n",
    "attack_pvalues[\"Time\"]=np.array([int(time.mktime(time.strptime(t, \" %d/%m/%Y %I:%M:%S %p\"))-time.mktime(base)) for t in np.array(dataAtck[\"Timestamp\"][1:])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_attack_true_false=(dataAtck[\"Normal/Attack\"]==\"Normal\")[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP_array=np.array([])\n",
    "FN_array=np.array([])\n",
    "FP_array=np.array([])\n",
    "TN_array=np.array([])\n",
    "\n",
    "no_attack_time_list=range(0,np.array(attack_pvalues[\"Time\"])[-1]+1)\n",
    "for x in attack_bins:\n",
    "    no_attack_time_list=[t for t in no_attack_time_list if t<x[0] or t>x[1]]\n",
    "num_no_attacks=len(no_attack_time_list)\n",
    "no_attack_dataset=attack_pvalues[no_attack_true_false]\n",
    "\n",
    "for p in np.arange(0,6000,10):\n",
    "    TP=0\n",
    "    FN=0\n",
    "    for x in attack_bins:\n",
    "        bin_dataset=attack_pvalues[\"Combined\"][np.multiply(np.array(attack_pvalues[\"Time\"]>=x[0]),np.array(attack_pvalues[\"Time\"]<=x[1]))]\n",
    "        if all(bin_dataset<p)==True:\n",
    "            FN=FN+1\n",
    "        else:\n",
    "            TP=TP+1\n",
    "    TP_array=np.hstack((TP_array,TP))\n",
    "    FN_array=np.hstack((FN_array,FN))\n",
    "    \n",
    "    no_attack_pvalues=no_attack_dataset[\"Combined\"]\n",
    "    FP=sum(np.array(no_attack_pvalues>p))\n",
    "    TN=num_no_attacks-FP\n",
    "    FP_array=np.hstack((FP_array,FP))\n",
    "    TN_array=np.hstack((TN_array,TN))\n",
    "    clear_output()\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditionPositive=TP_array+FN_array\n",
    "conditionNegative=TN_array+FP_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TPR_array=TP_array/conditionPositive\n",
    "FPR_array=FP_array/conditionNegative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(pickleFolder+'/FPR_LOG_FISH.pickle', 'wb') as handle:\n",
    "#    pickle.dump(FPR_array,handle)\n",
    "#with open(pickleFolder+'/TPR_LOG_FISH.pickle', 'wb') as handle:\n",
    "#    pickle.dump(TPR_array,handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "for m in attack_start_times:\n",
    "    plt.axvline(x=m,color=\"r\",linestyle=\"-\")\n",
    "twenty_sec=np.arange(0,no_attack_time_list[-1],20)\n",
    "plt.plot(attack_pvalues[\"Time\"],pvalue_dataframe_combineTime[\"Combined\"],\".b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(np.arange(0,1,0.0001),np.arange(0,1,0.0001),\"g\")\n",
    "plt.plot(FPR_array,TPR_array,\"k\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC curve LOGISTIC FISHER NO-BACKTRACK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_difference_list={}\n",
    "for col in Actuator_list:\n",
    "    prediction_matrix=dataNorm_reg_adpt.drop([col],axis=1).values\n",
    "    dupa=[]\n",
    "    for x in prediction_matrix:\n",
    "        m=d_log[\"classifier_{0}\".format(col)].predict(np.array(x).reshape(1,-1))\n",
    "        dupa.append(float(m))\n",
    "    dupa=np.array(dupa)\n",
    "    difference=dupa-np.array(dataNorm_reg_adpt[col])\n",
    "    log_difference_list[\"diff_{0}\".format(col)]=difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_attack_difference_list={}\n",
    "for col in Actuator_list:\n",
    "    prediction_matrix=dataAtck_reg_adpt.drop([col],axis=1).values\n",
    "    dupa=[]\n",
    "    for x in prediction_matrix:\n",
    "        m=d_log[\"classifier_{0}\".format(col)].predict(np.array(x).reshape(1,-1))\n",
    "        dupa.append(float(m))\n",
    "    dupa=np.array(dupa)\n",
    "    difference=dupa-np.array(dataAtck_reg_adpt[col])\n",
    "    log_attack_difference_list[\"diff_{0}\".format(col)]=difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_log_difference_list={}\n",
    "for col in rest_actuator:\n",
    "    difference=np.array(dataNorm_reg_adpt[col])-np.array(dataNorm_reg_adpt[col])\n",
    "    zero_log_difference_list[\"diff_{0}\".format(col)]=difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_log_attack_difference_list={}\n",
    "for col in rest_actuator:\n",
    "    difference=-np.array(dataAtck_reg_adpt[col])\n",
    "    zero_log_attack_difference_list[\"diff_{0}\".format(col)]=difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z= {**log_difference_list, **zero_log_difference_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m= {**log_attack_difference_list, **zero_log_attack_difference_list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "path=\"/Pickles\"\n",
    "pickleFolder=os.getcwd()+path\n",
    "\n",
    "#with open('z.pickle', 'wb') as handle:\n",
    "#    pickle.dump(z, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(pickleFolder+'/z.pickle', 'rb') as handle:\n",
    "    gg = pickle.load(handle)\n",
    "#with open('m.pickle', 'wb') as handle:\n",
    "#    pickle.dump(m, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(pickleFolder+'/m.pickle', 'rb') as handle:\n",
    "    ff = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#with open('log_attack_difference.pickle', 'wb') as handle:\n",
    "#    pickle.dump(log_attack_difference_list, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('log_attack_difference.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "log_attack_difference=b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_list={}\n",
    "for col in actuator_list:\n",
    "    plt.figure(figsize=(20,10))\n",
    "    num_bins=5\n",
    "    n, bins, patches = plt.hist(gg[\"diff_{0}\".format(col)], bins=np.arange(-2,4,1))\n",
    "    plt.title(\"Normal Histogram %s\" % col)\n",
    "    histogram_list[\"norm_hist_{0}\".format(col)]=n\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(20,10))\n",
    "    n, bins, patches = plt.hist(ff[\"diff_{0}\".format(col)], bins=np.arange(-2,4,1))\n",
    "    plt.title(\"Attack Histogram %s\" % col)\n",
    "    histogram_list[\"atck_hist_{0}\".format(col)]=n\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is just for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('histogram_list.pickle', 'wb') as handle:\n",
    "    pickle.dump(histogram_list, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#with open('histogram_list.pickle', 'rb') as handle:\n",
    "#    b = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to predict probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalIndex=list(dataAtck.iloc[list(dataAtck[\"Normal/Attack\"]==\"Normal\")].index)\n",
    "attackIndex=list(dataAtck.iloc[list(dataAtck[\"Normal/Attack\"]!=\"Normal\")].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#with open('difference.pickle', 'wb') as handle:\n",
    "#    pickle.dump(difference_list, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('difference.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "difference_list=b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('attack_difference.pickle', 'wb') as handle:\n",
    "    pickle.dump(attack_difference_list, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('attack_difference.pickle', 'rb') as handle:\n",
    "    c = pickle.load(handle)\n",
    "attack_difference_list=c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_data=pd.DataFrame([],columns=actuator_list)\n",
    "for col in actuator_list:\n",
    "    normalisedHist=histogram_list[\"norm_hist_{0}\".format(col)]/sum(histogram_list[\"norm_hist_{0}\".format(col)])\n",
    "    hist_data[col]=normalisedHist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pvalue_data=hist_data.copy()\n",
    "for col in pvalue_data.columns:\n",
    "    for i in range(0,len(hist_data)):\n",
    "        a=hist_data[col][i]\n",
    "        truthVec=(hist_data[col]<=a)\n",
    "        pvalue_data[col][i]=sum(np.multiply(hist_data[col],truthVec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_diff_data=pd.DataFrame([],columns=actuator_list).copy()\n",
    "for col in attack_diff_data:\n",
    "    attack_diff_data[col]=ff[\"diff_{0}\".format(col)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_ref_data=(attack_diff_data+2).copy()\n",
    "attack_pvalues=bin_ref_data.copy()\n",
    "for col in bin_ref_data:\n",
    "    print(col)\n",
    "    L=np.array([math.floor(x) for x in bin_ref_data[col]])\n",
    "    for i in range(0,len(bin_ref_data)):\n",
    "        try:\n",
    "            if pvalue_data[col][L[i]]!=0:\n",
    "                attack_pvalues.loc[:,col]=pvalue_data[col][L[i]]\n",
    "            else:\n",
    "                attack_pvalues.loc[:,col]=0.000001\n",
    "        except:\n",
    "            attack_pvalues.loc[:,col]=0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attack_pvalues.to_pickle(\"./logpvalues.pkl\")\n",
    "attack_pvalues = pd.read_pickle(\"./logpvalues.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attack_pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedValue=np.array([])\n",
    "for x in attack_pvalues.values:\n",
    "    combinedValue=np.hstack((combinedValue,sum(np.log(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_pvalues[\"Combined\"]=-2*combinedValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "#for m in attack_start_times:\n",
    "#    plt.axvline(x=m,color=\"r\",linestyle=\"-\")\n",
    "plt.plot(attack_pvalues[\"Combined\"],\".b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=open(\"/home/sharding/Documents/SummerProject/Data/AttackList_Indexed.csv\",\"r+\")\n",
    "file_atcklist=csv.reader(h)\n",
    "top=next(file_atcklist)\n",
    "attack_list=pd.DataFrame(file_atcklist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base=time.strptime('12/28/2015 10:00:00', '%m/%d/%Y %H:%M:%S')\n",
    "attack_start_times=[]\n",
    "for t in range(0,len(attack_list)):\n",
    "    attack_start_times.append(int(time.mktime(time.strptime(attack_list.iloc[t][1], '%m/%d/%Y %H:%M:%S'))-time.mktime(base)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_detect(t):\n",
    "    for T in range(t,t+1000):\n",
    "        if sum(np.array(attack_significance_data.iloc[T])!=0)>=5:\n",
    "            return(T-t)\n",
    "    return(\"No Error Detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_times=[]\n",
    "for k in attack_start_times:\n",
    "    detection_times.append(time_to_detect(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here the confusion matrix is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "detection_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=0\n",
    "for x in attack_significance_data.values:\n",
    "    b=(sum(np.array(x)!=0)>=7)\n",
    "    a=a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_significance_data[\"number of significant features\"]=np.array([15-sum(np.array(x)==0) for x in attack_significance_data.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attack_significance_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "plt.figure(figsize=(200,100))\n",
    "for m in attack_start_times:\n",
    "    plt.axvline(x=m,color=\"r\",linestyle=\"-\")\n",
    "plt.plot(list(attack_significance_data[\"number of significant features\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#>>> X, y = load_iris(return_X_y=True)\n",
    "y=dataNorm_reg_adpt[\"P101\"]\n",
    "X=dataNorm_reg_adpt.drop([\"P101\"],axis=1).copy().values\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial').fit(X, y)\n",
    "clf.predict(X[:2, :])\n",
    "\n",
    "#>>> clf.predict_proba(X[:2, :]) \n",
    "#array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
    "#       [9.7...e-01, 2.8...e-02, ...e-08]])\n",
    "#>>> clf.score(X, y)\n",
    "#0.97..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict_proba(X[:2]) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
